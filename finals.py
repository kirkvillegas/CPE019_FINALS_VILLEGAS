# -*- coding: utf-8 -*-
"""FINALS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YHrmQvJzSu15kLZlCpe-JzNnMFhbIvsI

NAME: KIRK PATRICK B. VILLEGAS <Br>
SUBJECT & SECTION: CPE019 - CPE32S1 <br>
INSTRUCTOR: ENGR. ROBIN VALENZUELA <br>
DATE SUBMITTED: JUNE 11,2025
"""

!pip install tensorflow streamlit opencv-python pillow

!pip install kaggle
!mkdir -p ~/.kaggle
!cp /path/to/your/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d pratik2901/multiclass-weather-dataset
!unzip multiclass-weather-dataset.zip

"""# **SPLITING DATA FOR TRAIN AND TEST**"""

import zipfile
import os
import shutil
from sklearn.model_selection import train_test_split

zip_path = "/content/drive/MyDrive/EMTECH 2 FINALS EXAM/archive.zip"
extract_dir = "/content/temp_extracted"
base_dir = "/content/weather_dataset"
classes = ['Cloudy', 'Rain', 'Shine', 'Sunrise']

if os.path.exists(extract_dir):
    shutil.rmtree(extract_dir)
os.makedirs(extract_dir, exist_ok=True)
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

def find_class_root(base, class_names):
    for root, dirs, files in os.walk(base):
        if all(cls in dirs for cls in class_names):
            return root
    return None

original_dataset_dir = find_class_root(extract_dir, classes)
if not original_dataset_dir:
    raise Exception("Could not find a directory containing all class folders.")

for split in ['train', 'test']:
    for c in classes:
        os.makedirs(os.path.join(base_dir, split, c), exist_ok=True)

for c in classes:
    src_dir = os.path.join(original_dataset_dir, c)
    all_images = [f for f in os.listdir(src_dir) if os.path.isfile(os.path.join(src_dir, f))]
    train_files, test_files = train_test_split(all_images, test_size=0.2, random_state=42)
    for f in train_files:
        shutil.copy(os.path.join(src_dir, f), os.path.join(base_dir, 'train', c, f))
    for f in test_files:
        shutil.copy(os.path.join(src_dir, f), os.path.join(base_dir, 'test', c, f))

shutil.rmtree(extract_dir)

"""# **AUGMENTED CNN**"""

import zipfile
import os
import shutil
import random
from tensorflow.keras.preprocessing.image import ImageDataGenerator

class_names = ["Cloudy", "Rain", "Shine", "Sunrise"]
zip_path = "/content/drive/MyDrive/EMTECH 2 FINALS EXAM/archive.zip"
extract_dir = "/content/temp_dataset/"
output_train = "train"
output_test = "test"

def print_dir_tree(start_path, max_depth=3):
    print(f"Directory tree starting from: {start_path}")
    for root, dirs, files in os.walk(start_path):
        level = root.replace(start_path, '').count(os.sep)
        if level >= max_depth:
            continue
        indent = ' ' * 2 * (level)
        print(f"{indent}{os.path.basename(root)}/")
        for f in files[:5]:
            print(f"{indent}  {f}")

if os.path.exists(extract_dir):
    shutil.rmtree(extract_dir)
os.makedirs(extract_dir, exist_ok=True)


with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print("Directory structure after extraction:")
print_dir_tree(extract_dir, max_depth=4)

def find_class_root(base, class_names):
    for root, dirs, files in os.walk(base):
        if all(cls in dirs for cls in class_names):
            for cls in class_names:
                class_path = os.path.join(root, cls)
                if not os.path.isdir(class_path) or not any(os.path.isfile(os.path.join(class_path, f)) for f in os.listdir(class_path)):
                    continue
            return root
    return None

real_base = find_class_root(extract_dir, class_names)
if real_base is None:
    raise Exception("Could not find a directory containing all class folders. Please check the printed directory tree above to adjust class_names or extract_dir.")
else:
    print(f"\nFound class root directory: {real_base}")

for d in [output_train, output_test]:
    if os.path.exists(d):
        shutil.rmtree(d)
    os.makedirs(d, exist_ok=True)
    for class_name in class_names:
        os.makedirs(os.path.join(d, class_name), exist_ok=True)

for class_name in class_names:
    class_dir = os.path.join(real_base, class_name)
    if not os.path.isdir(class_dir):
        print(f"Warning: {class_dir} does not exist in extracted data!")
        continue
    images = [img for img in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, img))]
    random.shuffle(images)
    split_idx = int(0.8 * len(images))
    train_images = images[:split_idx]
    test_images = images[split_idx:]
    for img in train_images:
        dest_dir = os.path.join(output_train, class_name)
        os.makedirs(dest_dir, exist_ok=True)
        shutil.move(os.path.join(class_dir, img), os.path.join(dest_dir, img))
    for img in test_images:
        dest_dir = os.path.join(output_test, class_name)
        os.makedirs(dest_dir, exist_ok=True)
        shutil.move(os.path.join(class_dir, img), os.path.join(dest_dir, img))

if extract_dir != "./" and extract_dir != ".":
    shutil.rmtree(extract_dir)
print("Split complete! Check 'train/' and 'test/' folders.")
print("Classes in train:", os.listdir(output_train))
print("Classes in test:", os.listdir(output_test))


img_width, img_height = 75, 75
batch_size = 32
train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)
train_generator = train_datagen.flow_from_directory(
    output_train,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
    classes=class_names
)
test_generator = test_datagen.flow_from_directory(
    output_test,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',
    classes=class_names
)
print("Image data generators created successfully.")
print("Detected classes:", train_generator.class_indices)

model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(75,75,3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(4, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks
early_stop = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)
checkpoint = ModelCheckpoint("best_model.h5", monitor='val_accuracy', save_best_only=True)

history = model.fit(train_generator, validation_data=test_generator, epochs=30, callbacks=[early_stop, checkpoint])

# Save final model
model.save("Weather_model.h5")

# Evaluate
loss, acc = model.evaluate(test_generator)
print(f"Final Accuracy: {acc * 100:.2f}%")